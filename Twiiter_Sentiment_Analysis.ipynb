{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMDgp6IyRKbCLUdkGuYI3Iu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bijitdeb/Course/blob/master/Twiiter_Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#libraries being used\n",
        "#re: for using Regex Expression\n",
        "#np, pd : python libraries\n",
        "#Nltk : Natural Language Toolkit for language processing\n",
        "#sklearn: for using required machine learning models\n",
        "#seaborn: python data visualization based on metaplotlib"
      ],
      "metadata": {
        "id": "CZVtzBWa8brS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "# plotting\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "# nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "import nltk\n",
        "from bs4 import BeautifulSoup\n",
        "import re,string\n",
        "# sklearn\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import confusion_matrix, classification_report"
      ],
      "metadata": {
        "id": "gqlh1LtZ8lNt"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Importing the dataset\n",
        "DATASET_COLUMNS=['target','ids','date','flag','user','text']\n",
        "DATASET_ENCODING = \"ISO-8859-1\"\n",
        "df = pd.read_csv('/content/drive/My Drive/Twitter Feed/Project_Data.csv', encoding=DATASET_ENCODING, names=DATASET_COLUMNS)\n",
        "df.tail(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "r60N3Cus94Dg",
        "outputId": "a0665aa3-de5a-4fad-c62c-18407c24a159"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         target         ids                          date      flag  \\\n",
              "1599995       4  2193601966  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
              "1599996       4  2193601969  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
              "1599997       4  2193601991  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
              "1599998       4  2193602064  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
              "1599999       4  2193602129  Tue Jun 16 08:40:50 PDT 2009  NO_QUERY   \n",
              "\n",
              "                    user                                               text  \n",
              "1599995  AmandaMarie1028  Just woke up. Having no school is the best fee...  \n",
              "1599996      TheWDBoards  TheWDB.com - Very cool to hear old Walt interv...  \n",
              "1599997           bpbabe  Are you ready for your MoJo Makeover? Ask me f...  \n",
              "1599998     tinydiamondz  Happy 38th Birthday to my boo of alll time!!! ...  \n",
              "1599999   RyanTrevMorris  happy #charitytuesday @theNSPCC @SparksCharity...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9310f3b4-4238-4d83-8b51-52ee26bea25f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>target</th>\n",
              "      <th>ids</th>\n",
              "      <th>date</th>\n",
              "      <th>flag</th>\n",
              "      <th>user</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1599995</th>\n",
              "      <td>4</td>\n",
              "      <td>2193601966</td>\n",
              "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>AmandaMarie1028</td>\n",
              "      <td>Just woke up. Having no school is the best fee...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1599996</th>\n",
              "      <td>4</td>\n",
              "      <td>2193601969</td>\n",
              "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>TheWDBoards</td>\n",
              "      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1599997</th>\n",
              "      <td>4</td>\n",
              "      <td>2193601991</td>\n",
              "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>bpbabe</td>\n",
              "      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1599998</th>\n",
              "      <td>4</td>\n",
              "      <td>2193602064</td>\n",
              "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>tinydiamondz</td>\n",
              "      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1599999</th>\n",
              "      <td>4</td>\n",
              "      <td>2193602129</td>\n",
              "      <td>Tue Jun 16 08:40:50 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>RyanTrevMorris</td>\n",
              "      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9310f3b4-4238-4d83-8b51-52ee26bea25f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9310f3b4-4238-4d83-8b51-52ee26bea25f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9310f3b4-4238-4d83-8b51-52ee26bea25f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Next, let’s get the count of the null values before proceeding to the pre-processing of the text. You will see that the number of null values is zero, so there is no need to clean the data set.\n",
        "\n",
        "np.sum(df.isnull().any(axis=1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MYppnJJi-VYz",
        "outputId": "1094d0a6-0284-4c97-ed8a-2e80db81762c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let’s get a sense of the data set. The command below will help you understand what the data set looks like\n",
        "\n",
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bjtmWkj7-gkL",
        "outputId": "9b72216e-1786-4ea0-9b04-2c5e63d9fcc0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1600000 entries, 0 to 1599999\n",
            "Data columns (total 6 columns):\n",
            " #   Column  Non-Null Count    Dtype \n",
            "---  ------  --------------    ----- \n",
            " 0   target  1600000 non-null  int64 \n",
            " 1   ids     1600000 non-null  int64 \n",
            " 2   date    1600000 non-null  object\n",
            " 3   flag    1600000 non-null  object\n",
            " 4   user    1600000 non-null  object\n",
            " 5   text    1600000 non-null  object\n",
            "dtypes: int64(2), object(4)\n",
            "memory usage: 73.2+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We mainly need to process the data for the tweet column, which is the text column, and the target column, which is the negative/positive sentiment. Let’s take the two columns for further analysis.\n",
        "\n",
        "data=df[['text','target']]"
      ],
      "metadata": {
        "id": "_QVTJLRY-yRw"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Further, we know that the positive tweet is represented by “4.” Let’s replace it with 1. So, now the positive tweet is represented by 1 and the negative tweet by 0.\n",
        "\n",
        "data['target'] = data['target'].replace(4,1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xsAKFHY-3RV",
        "outputId": "c4c26874-5a79-4d07-a909-5aa12f58d8ab"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-c350880468d7>:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data['target'] = data['target'].replace(4,1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, let’s separate the positive and negative tweets and take only 10,000 rows to perform the analysis\n",
        "\n",
        "data_pos = data[data['target'] == 1]\n",
        "data_neg = data[data['target'] == 0]\n",
        "\n",
        "data_pos = data_pos.iloc[:int(10000)]\n",
        "data_neg = data_neg.iloc[:int(10000)]"
      ],
      "metadata": {
        "id": "Psbgbfru_E8T"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Further, let’s combine these 10,000 positive and negative tweets. We segregated these tweets to get the equal amount of both positive and negative tweets and hence ensured that the training of the model is not biased.\n",
        "\n",
        "dataset = pd.concat([data_pos, data_neg])"
      ],
      "metadata": {
        "id": "5CGiY0_A_USV"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let’s now convert the text into lowercase to eliminate the casing variation.\n",
        "\n",
        "dataset['text']=dataset['text'].str.lower()\n",
        "dataset['text'].tail()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRNbZ76g_YfE",
        "outputId": "3fdd110a-3858-48c3-e750-1630b0034f5c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9995                                      aww that's sad \n",
              "9996      stupid dvds stuffing up the good bits in jaws. \n",
              "9997    @dandy_sephy no. only close friends and family...\n",
              "9998    crap! after looking when i last tweeted... why...\n",
              "9999                            its another rainboot day \n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let’s now define a set of stop words, which will be removed from the text, as they are of the least importance and are just present for grammatical correctness.\n",
        "\n",
        "stopwordlist = ['a', 'about', 'above', 'after', 'again', 'ain', 'all', 'am', 'an',\n",
        "             'and','any','are', 'as', 'at', 'be', 'because', 'been', 'before',\n",
        "             'being', 'below', 'between','both', 'by', 'can', 'd', 'did', 'do',\n",
        "             'does', 'doing', 'down', 'during', 'each','few', 'for', 'from',\n",
        "             'further', 'had', 'has', 'have', 'having', 'he', 'her', 'here',\n",
        "             'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', 'in',\n",
        "             'into','is', 'it', 'its', 'itself', 'just', 'll', 'm', 'ma',\n",
        "             'me', 'more', 'most','my', 'myself', 'now', 'o', 'of', 'on', 'once',\n",
        "             'only', 'or', 'other', 'our', 'ours','ourselves', 'out', 'own', 're','s', 'same', 'she', \"shes\", 'should', \"shouldve\",'so', 'some', 'such',\n",
        "             't', 'than', 'that', \"thatll\", 'the', 'their', 'theirs', 'them',\n",
        "             'themselves', 'then', 'there', 'these', 'they', 'this', 'those',\n",
        "             'through', 'to', 'too','under', 'until', 'up', 've', 'very', 'was',\n",
        "             'we', 'were', 'what', 'when', 'where','which','while', 'who', 'whom',\n",
        "             'why', 'will', 'with', 'won', 'y', 'you', \"youd\",\"youll\", \"youre\",\n",
        "             \"youve\", 'your', 'yours', 'yourself', 'yourselves']"
      ],
      "metadata": {
        "id": "hCBWab7q_j9s"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, remove the stop words from the text column.\n",
        "\n",
        "STOPWORDS = set(stopwordlist)\n",
        "def cleaning_stopwords(text):\n",
        "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
        "dataset['text'] = dataset['text'].apply(lambda text: cleaning_stopwords(text))\n",
        "dataset['text'].tail()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ibnH5kiL_skO",
        "outputId": "a86e2832-6230-40fb-e601-6fde7009d9d4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9995                                       aww that's sad\n",
              "9996                 stupid dvds stuffing good bits jaws.\n",
              "9997    @dandy_sephy no. close friends family i'm afra...\n",
              "9998    crap! looking last tweeted... early. it's 10! ...\n",
              "9999                                 another rainboot day\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, let’s use the concept of regular expression to remove the URLs and numbers from the text to clean it further.\n",
        "\n",
        "def cleaning_URLs(data):\n",
        "    return re.sub('((www.[^s]+)|(https?://[^s]+))',' ',data)\n",
        "dataset['text'] = dataset['text'].apply(lambda x: cleaning_URLs(x))\n",
        "dataset['text'].head()\n",
        "\n",
        "def cleaning_numbers(data):\n",
        "    return re.sub('[0-9]+', '', data)\n",
        "dataset['text'] = dataset['text'].apply(lambda x: cleaning_numbers(x))\n",
        "dataset['text'].head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xbtsmPJW_xZm",
        "outputId": "e097b689-a107-4958-8c40-4f119f7552b5"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "800000                 love @healthuandpets u guys r best!!\n",
              "800001    im meeting one besties tonight! cant wait!! - ...\n",
              "800002    @darealsunisakim thanks twitter add, sunisa! g...\n",
              "800003    sick really cheap hurts much eat real food plu...\n",
              "800004                       @lovesbrooklyn effect everyone\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, we must tokenize the texts. This can be done using the NLTK library\n",
        "\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "dataset['text'] = dataset['text'].apply(tokenizer.tokenize)\n",
        "dataset['text'].tail()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ryWiMurCAVIE",
        "outputId": "48819efe-00cf-476c-8827-13b19682692c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9995                                  [aww, that, s, sad]\n",
              "9996           [stupid, dvds, stuffing, good, bits, jaws]\n",
              "9997    [dandy_sephy, no, close, friends, family, i, m...\n",
              "9998    [crap, looking, last, tweeted, early, it, s, s...\n",
              "9999                             [another, rainboot, day]\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now let’s apply stemming and lemmatization, which brings the words in various forms to their base form\n",
        "\n",
        "#stemming\n",
        "import nltk\n",
        "st = nltk.PorterStemmer()\n",
        "def stemming_on_text(data):\n",
        "    text = [st.stem(word) for word in data]\n",
        "    return data\n",
        "dataset['text']= dataset['text'].apply(lambda x: stemming_on_text(x))\n",
        "dataset['text'].head()\n",
        "\n",
        "#Lemmatization\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "lm = nltk.WordNetLemmatizer()\n",
        "def lemmatizer_on_text(data):\n",
        "    text = [lm.lemmatize(word) for word in data]\n",
        "    return data\n",
        "dataset['text'] = dataset['text'].apply(lambda x: lemmatizer_on_text(x))\n",
        "dataset['text'].head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZCVcnugAZpH",
        "outputId": "c144f2d9-71ca-48cd-cf07-049b6ca3cd70"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "800000             [love, healthuandpets, u, guys, r, best]\n",
              "800001    [im, meeting, one, besties, tonight, cant, wai...\n",
              "800002    [darealsunisakim, thanks, twitter, add, sunisa...\n",
              "800003    [sick, really, cheap, hurts, much, eat, real, ...\n",
              "800004                    [lovesbrooklyn, effect, everyone]\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's get started with the model building:"
      ],
      "metadata": {
        "id": "67aV-iJ4At2T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let’s separate X and Y, the input feature, and the label.\n",
        "\n",
        "X=data.text\n",
        "\n",
        "y=data.target"
      ],
      "metadata": {
        "id": "eL_dyLXoAv1G"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The next step involves splitting the data into train and test data sets. We are splitting it in a 95 to 5 ratio.\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.05, random_state =26105111)"
      ],
      "metadata": {
        "id": "G9SLMByJA_Nu"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now using the TFIDF method, we will get the feature words, which will be the basis for building the model.\n",
        "\n",
        "vectoriser = TfidfVectorizer(ngram_range=(1,2), max_features=500000)\n",
        "\n",
        "vectoriser.fit(X_train)\n",
        "\n",
        "print('No. of feature_words: ', len(vectoriser.get_feature_names_out()))"
      ],
      "metadata": {
        "id": "ENR-yb4TBEJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, we will transform X_train and X_test using the TFIDF vectorizer\n",
        "\n",
        "vectoriser = TfidfVectorizer(ngram_range=(1,2), max_features=500000)\n",
        "\n",
        "vectoriser.fit(X_train)\n",
        "\n",
        "print('No. of feature_words: ', len(vectoriser.get_feature_names()))\n"
      ],
      "metadata": {
        "id": "7GztfznxBNSg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Before starting the model-building and evaluation process, let’s define a function that will perform model evaluation. We will evaluate based on precision, recall, confusion matrix, etc. This is just evaluation code; you need not understand it in detail.\n",
        "\n",
        "def model_Evaluate(model):\n",
        "\n",
        "  # Predict values for Test dataset\n",
        "\n",
        "  y_pred = model.predict(X_test)\n",
        "\n",
        "  # Print the evaluation metrics for the dataset.\n",
        "\n",
        "  print(classification_report(y_test, y_pred))\n",
        "\n",
        "  # Compute and plot the Confusion matrix\n",
        "\n",
        "  cf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "  categories = ['Negative','Positive']\n",
        "\n",
        "  group_names = ['True Neg','False Pos', 'False Neg','True Pos']\n",
        "\n",
        "  group_percentages = ['{0:.2%}'.format(value) for value in cf_matrix.flatten() / np.sum(cf_matrix)]\n",
        "\n",
        "  labels = [f'{v1}n{v2}' for v1, v2 in zip(group_names,group_percentages)]\n",
        "\n",
        "  labels = np.asarray(labels).reshape(2,2)\n",
        "\n",
        "  sns.heatmap(cf_matrix, annot = labels, cmap = 'Blues',fmt = '',\n",
        "\n",
        "  xticklabels = categories, yticklabels = categories)\n",
        "\n",
        "  plt.xlabel(\"Predicted values\", fontdict = {'size':14}, labelpad = 10)\n",
        "\n",
        "  plt.ylabel(\"Actual values\" , fontdict = {'size':14}, labelpad = 10)\n",
        "\n",
        "  plt.title (\"Confusion Matrix\", fontdict = {'size':18}, pad = 20)"
      ],
      "metadata": {
        "id": "Zdvhyfm7BWIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Next, let’s use the Bernoulli Naive Bayes model for model-building. Let’s fit the model.\n",
        "# About Naive Bayes: The Naïve Bayes algorithm is a supervised learning algorithm based on the Bayes’ theorem. It is used for solving classification problems.\n",
        "\n",
        "BNBmodel = BernoulliNB()\n",
        "\n",
        "BNBmodel.fit(X_train, y_train)\n",
        "\n",
        "model_Evaluate(BNBmodel)\n",
        "\n",
        "y_pred1 = BNBmodel.predict(X_test)"
      ],
      "metadata": {
        "id": "gKcoEG7ZBfs6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}